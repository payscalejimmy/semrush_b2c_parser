This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
payscale_parser.py
README.md
run_parser.py
setup.py
test_parser.py
url_analysis_results.md

================================================================
Files
================================================================

================
File: payscale_parser.py
================
import pandas as pd
import re
from urllib.parse import urlparse, unquote
import numpy as np
from collections import defaultdict

class PayScaleURLParser:
    def __init__(self):
        # More specific patterns for better parsing
        self.url_patterns = {
            'cost_of_living': re.compile(r'/cost-of-living-calculator/([^/]+)(?:/(.+))?'),
            'research_job': re.compile(r'/research/([^/]+)/Job=([^/]+)/(.+)'),
            'research_employer': re.compile(r'/research/([^/]+)/Employer=([^/]+)/(.+)'),
            'research_country': re.compile(r'/research/([^/]+)/Country=([^/]+)/(.+)'),
            'research_skill': re.compile(r'/research/([^/]+)/Skill=([^/]+)/(.+)'),
            'general_research': re.compile(r'/research/([^/]+)/?$'),
        }
        
        # Patterns for parsing the metric portion of URLs
        self.metric_patterns = {
            # Salary/Page-3, Hourly_Rate/Page-2, etc.
            'pagination': re.compile(r'^([^/]+)/Page-(\d+)$'),
            
            # Hourly_Rate/0a9d4bb0/H.E.B., Salary/ffed3159/Additional_Info
            'with_id_and_extra': re.compile(r'^([^/]+)/([^/]+)/(.+)$'),
            
            # Just the metric type: Salary, Hourly_Rate, etc.
            'simple': re.compile(r'^([^/]+)$'),
            
            # City/State pattern: City/Washington-DC/Page-2
            'city_location': re.compile(r'^City/([^/]+)(?:/Page-(\d+))?$'),
        }
    
    def parse_metric_portion(self, metric_portion):
        """
        Parse the metric portion of research URLs to extract:
        - metric_type (Salary, Hourly_Rate, etc.)
        - page_number (if pagination exists)
        - additional_employer (if present)
        - location_info (if present)
        - unique_id (if present)
        """
        result = {
            'metric_type': None,
            'page_number': None,
            'additional_employer': None,
            'location_info': None,
            'unique_id': None,
            'raw_metric': metric_portion
        }
        
        if not metric_portion:
            return result
        
        # Check for pagination pattern first (Salary/Page-3)
        pagination_match = self.metric_patterns['pagination'].match(metric_portion)
        if pagination_match:
            result['metric_type'] = pagination_match.group(1)
            result['page_number'] = int(pagination_match.group(2))
            return result
        
        # Check for city location pattern (City/Washington-DC/Page-2)
        city_match = self.metric_patterns['city_location'].match(metric_portion)
        if city_match:
            result['metric_type'] = 'City'  # This is a location-based research page
            result['location_info'] = unquote(city_match.group(1)).replace('-', ' ')
            if city_match.group(2):
                result['page_number'] = int(city_match.group(2))
            return result
        
        # Check for pattern with ID and additional info (Hourly_Rate/0a9d4bb0/H.E.B.)
        id_extra_match = self.metric_patterns['with_id_and_extra'].match(metric_portion)
        if id_extra_match:
            result['metric_type'] = id_extra_match.group(1)
            
            # The middle part could be an ID or location
            middle_part = id_extra_match.group(2)
            extra_part = unquote(id_extra_match.group(3)).replace('_', ' ').replace('%2C', ',')
            
            # If middle part looks like an ID (alphanumeric), treat as ID
            if re.match(r'^[a-f0-9]{8,}$', middle_part, re.I):
                result['unique_id'] = middle_part
                result['additional_employer'] = extra_part
            # If it looks like a location pattern (contains dashes or state codes)
            elif '-' in middle_part or len(middle_part) == 2:
                result['location_info'] = middle_part.replace('-', ' ')
                # The extra part might be additional location or employer info
                if any(word in extra_part.lower() for word in ['page', 'salary', 'hourly']):
                    # This might be additional URL structure
                    pass
                else:
                    result['additional_employer'] = extra_part
            else:
                # Default: treat as additional employer info
                result['additional_employer'] = f"{middle_part} {extra_part}".replace('_', ' ')
            
            return result
        
        # Simple metric type only
        simple_match = self.metric_patterns['simple'].match(metric_portion)
        if simple_match:
            result['metric_type'] = simple_match.group(1)
            return result
        
        # Fallback: split on / and try to parse components
        parts = metric_portion.split('/')
        if len(parts) >= 1:
            result['metric_type'] = parts[0]
            
            if len(parts) >= 2:
                # Check if second part is a page number
                if parts[1].startswith('Page-'):
                    result['page_number'] = int(parts[1].split('-')[1])
                else:
                    result['unique_id'] = parts[1]
            
            if len(parts) >= 3:
                result['additional_employer'] = unquote(parts[2]).replace('_', ' ').replace('%2C', ',')
        
        return result
    
    def parse_url(self, url):
        """
        Parse a PayScale URL and extract relevant components
        Returns a dictionary with enhanced categorization info
        """
        parsed = urlparse(url)
        path = parsed.path
        
        result = {
            'url': url,
            'domain': parsed.netloc,
            'full_path': path,
            'section': None,
            'subsection': None,
            'location_state': None,
            'location_city': None,
            'country': None,
            'job_title': None,
            'employer': None,
            'skill': None,
            'metric_type': None,
            'page_number': None,
            'additional_employer': None,
            'location_info': None,
            'unique_id': None,
            'category': 'other'
        }
        
        # Cost of living calculator
        if '/cost-of-living-calculator' in path:
            result['section'] = 'cost_of_living'
            result['category'] = 'cost_of_living'
            
            match = self.url_patterns['cost_of_living'].search(path)
            if match:
                location = match.group(1)
                if location:
                    # Parse state-city format
                    parts = location.split('-', 1)
                    if len(parts) >= 2:
                        result['location_state'] = parts[0].replace('-', ' ')
                        result['location_city'] = parts[1].replace('-', ' ')
                    else:
                        result['location_state'] = parts[0].replace('-', ' ')
        
        # Research sections
        elif '/research/' in path:
            result['section'] = 'research'
            
            # Job research
            match = self.url_patterns['research_job'].search(path)
            if match:
                result['category'] = 'research_job'
                result['country'] = match.group(1)
                result['job_title'] = unquote(match.group(2)).replace('_', ' ').replace('%2C', ',')
                
                # Parse the metric portion with enhanced logic
                metric_info = self.parse_metric_portion(match.group(3))
                result.update(metric_info)
            
            # Employer research
            elif self.url_patterns['research_employer'].search(path):
                match = self.url_patterns['research_employer'].search(path)
                result['category'] = 'research_employer'
                result['country'] = match.group(1)
                result['employer'] = unquote(match.group(2)).replace('_', ' ').replace('%2C', ',')
                
                # Parse the metric portion with enhanced logic
                metric_info = self.parse_metric_portion(match.group(3))
                result.update(metric_info)
            
            # Country research
            elif self.url_patterns['research_country'].search(path):
                match = self.url_patterns['research_country'].search(path)
                result['category'] = 'research_country'
                result['country'] = match.group(1)
                result['subsection'] = unquote(match.group(2)).replace('_', ' ')
                
                # Parse the metric portion
                metric_info = self.parse_metric_portion(match.group(3))
                result.update(metric_info)
            
            # Skill research
            elif self.url_patterns['research_skill'].search(path):
                match = self.url_patterns['research_skill'].search(path)
                result['category'] = 'research_skill'
                result['country'] = match.group(1)
                result['skill'] = unquote(match.group(2)).replace('_', ' ').replace('%2C', ',')
                
                # Parse the metric portion
                metric_info = self.parse_metric_portion(match.group(3))
                result.update(metric_info)
            
            # General research
            elif self.url_patterns['general_research'].search(path):
                match = self.url_patterns['general_research'].search(path)
                result['category'] = 'research_general'
                result['country'] = match.group(1)
        
        # Other sections (unchanged)
        elif path == '/' or path == '':
            result['section'] = 'homepage'
            result['category'] = 'homepage'
        elif '/salary-calculator' in path:
            result['section'] = 'salary_calculator'
            result['category'] = 'tools'
        elif '/products/' in path:
            result['section'] = 'products'
            result['category'] = 'commercial'
        elif '/careers' in path:
            result['section'] = 'careers'
            result['category'] = 'corporate'
        elif '/compensation-trends/' in path:
            result['section'] = 'compensation_trends'
            result['category'] = 'content'
        else:
            result['section'] = 'other'
            result['category'] = 'other'
            
        return result
    
    def parse_csv_data(self, csv_data, url_column='URL'):
        """
        Parse CSV data and add URL categorization with enhanced metric parsing
        """
        # If csv_data is a string, convert to DataFrame
        if isinstance(csv_data, str):
            # Handle the case where data might be tab or comma separated
            if '\t' in csv_data:
                df = pd.read_csv(pd.StringIO(csv_data), sep='\t')
            else:
                df = pd.read_csv(pd.StringIO(csv_data))
        else:
            df = csv_data.copy()
        
        # Parse each URL
        parsed_data = []
        for idx, row in df.iterrows():
            url = row[url_column] if url_column in row else row.iloc[0]
            parsed_url = self.parse_url(url)
            
            # Combine original data with parsed data
            combined = dict(row)
            combined.update(parsed_url)
            parsed_data.append(combined)
        
        return pd.DataFrame(parsed_data)
    
    def analyze_traffic_by_category(self, df, traffic_column='Traffic'):
        """
        Enhanced traffic analysis including new fields
        """
        analyses = {}
        
        # Traffic by main section
        section_traffic = df.groupby('section')[traffic_column].agg(['sum', 'count', 'mean']).round(2)
        section_traffic.columns = ['Total_Traffic', 'URL_Count', 'Avg_Traffic']
        analyses['by_section'] = section_traffic.sort_values('Total_Traffic', ascending=False)
        
        # Traffic by category
        category_traffic = df.groupby('category')[traffic_column].agg(['sum', 'count', 'mean']).round(2)
        category_traffic.columns = ['Total_Traffic', 'URL_Count', 'Avg_Traffic']
        analyses['by_category'] = category_traffic.sort_values('Total_Traffic', ascending=False)
        
        # Traffic by metric type (now properly parsed)
        metric_df = df[df['metric_type'].notna()].copy()
        if not metric_df.empty:
            metric_traffic = metric_df.groupby('metric_type')[traffic_column].agg(['sum', 'count', 'mean']).round(2)
            metric_traffic.columns = ['Total_Traffic', 'URL_Count', 'Avg_Traffic']
            analyses['by_metric_type'] = metric_traffic.sort_values('Total_Traffic', ascending=False)
        
        # Traffic by page number (pagination analysis)
        page_df = df[df['page_number'].notna()].copy()
        if not page_df.empty:
            page_traffic = page_df.groupby('page_number')[traffic_column].agg(['sum', 'count', 'mean']).round(2)
            page_traffic.columns = ['Total_Traffic', 'URL_Count', 'Avg_Traffic']
            analyses['by_page_number'] = page_traffic.sort_values('page_number')
        
        # Cost of living by state
        col_df = df[df['category'] == 'cost_of_living'].copy()
        if not col_df.empty:
            state_traffic = col_df.groupby('location_state')[traffic_column].agg(['sum', 'count', 'mean']).round(2)
            state_traffic.columns = ['Total_Traffic', 'URL_Count', 'Avg_Traffic']
            analyses['cost_of_living_by_state'] = state_traffic.sort_values('Total_Traffic', ascending=False)
        
        # Research by country
        research_df = df[df['section'] == 'research'].copy()
        if not research_df.empty:
            country_traffic = research_df.groupby('country')[traffic_column].agg(['sum', 'count', 'mean']).round(2)
            country_traffic.columns = ['Total_Traffic', 'URL_Count', 'Avg_Traffic']
            analyses['research_by_country'] = country_traffic.sort_values('Total_Traffic', ascending=False)
        
        # Top employers by traffic (now includes additional_employer)
        employer_df = df[df['category'] == 'research_employer'].copy()
        if not employer_df.empty:
            employer_traffic = employer_df.groupby('employer')[traffic_column].agg(['sum', 'count', 'mean']).round(2)
            employer_traffic.columns = ['Total_Traffic', 'URL_Count', 'Avg_Traffic']
            analyses['top_employers'] = employer_traffic.sort_values('Total_Traffic', ascending=False).head(20)
        
        # Additional employers analysis (from the parsed additional_employer field)
        additional_emp_df = df[df['additional_employer'].notna()].copy()
        if not additional_emp_df.empty:
            add_emp_traffic = additional_emp_df.groupby('additional_employer')[traffic_column].agg(['sum', 'count', 'mean']).round(2)
            add_emp_traffic.columns = ['Total_Traffic', 'URL_Count', 'Avg_Traffic']
            analyses['additional_employers'] = add_emp_traffic.sort_values('Total_Traffic', ascending=False).head(20)
        
        # Top jobs by traffic
        job_df = df[df['category'] == 'research_job'].copy()
        if not job_df.empty:
            job_traffic = job_df.groupby('job_title')[traffic_column].agg(['sum', 'count', 'mean']).round(2)
            job_traffic.columns = ['Total_Traffic', 'URL_Count', 'Avg_Traffic']
            analyses['top_jobs'] = job_traffic.sort_values('Total_Traffic', ascending=False).head(20)
        
        return analyses
    
    def export_parsed_data(self, df, filename='parsed_payscale_data.csv'):
        """
        Export parsed data to CSV with all new columns
        """
        df.to_csv(filename, index=False)
        print(f"Data exported to {filename}")
        return filename

# Test with your problematic URLs
def test_improved_parsing():
    """Test the improved parser with the problematic URLs"""
    
    test_urls = [
        "https://www.payscale.com/research/US/Employer=University_of_Michigan_(U-M)/Salary/Page-3",
        "https://www.payscale.com/research/US/Job=Night_Stocker/Hourly_Rate/0a9d4bb0/H.E.B.",
        "https://www.payscale.com/research/US/Job=Accountant/Salary/ffed3159/Portland-OR",
        "https://www.payscale.com/research/US/Employer=U.S._House_of_Representatives/City/Washington-DC/Page-2",
        "https://www.payscale.com/research/US/Employer=The_Heritage_Foundation/City/Washington-DC/Page-2"
    ]
    
    parser = PayScaleURLParser()
    
    print("Testing improved URL parsing:")
    print("=" * 80)
    
    for url in test_urls:
        result = parser.parse_url(url)
        print(f"\nURL: {url}")
        print(f"Employer: {result['employer']}")
        print(f"Job Title: {result['job_title']}")
        print(f"Metric Type: {result['metric_type']}")
        print(f"Page Number: {result['page_number']}")
        print(f"Additional Employer: {result['additional_employer']}")
        print(f"Location Info: {result['location_info']}")
        print(f"Unique ID: {result['unique_id']}")
        print(f"Category: {result['category']}")
        print("-" * 40)

# Additional functions for backward compatibility and large file processing
def batch_process_large_file(filename, batch_size=10000, url_column='URL'):
    """
    Process large CSV files in batches for memory efficiency
    """
    parser = PayScaleURLParser()
    
    # Process file in chunks
    chunk_list = []
    chunk_count = 0
    
    print(f"Processing file in batches of {batch_size:,} rows...")
    
    try:
        for chunk in pd.read_csv(filename, chunksize=batch_size):
            chunk_count += 1
            print(f"Processing batch {chunk_count} ({len(chunk):,} rows)...")
            parsed_chunk = parser.parse_csv_data(chunk, url_column)
            chunk_list.append(parsed_chunk)
        
        print(f"Combining {len(chunk_list)} batches...")
        # Combine all chunks
        full_df = pd.concat(chunk_list, ignore_index=True)
        return full_df
        
    except Exception as e:
        print(f"Error processing file in batches: {e}")
        raise

def process_url_list(url_list):
    """
    Process a list of URLs and return a DataFrame with parsed information
    Standalone function for backward compatibility
    """
    parser = PayScaleURLParser()
    return parser.parse_url_list(url_list)

def process_payscale_urls():
    """
    Example function to process the PayScale URLs from your document
    """
    # URLs from your document
    urls = [
        "https://www.payscale.com/",
        "https://www.payscale.com/cost-of-living-calculator/Maryland-Rockville",
        "https://www.payscale.com/cost-of-living-calculator",
        "https://www.payscale.com/cost-of-living-calculator/Washington-Seattle",
        "https://www.payscale.com/research/US/Job",
        "https://www.payscale.com/cost-of-living-calculator/Texas-Frisco",
        "https://www.payscale.com/research/US/Employer=U.S._Postal_Service_(USPS)/Salary",
        "https://www.payscale.com/cost-of-living-calculator/California-Carlsbad",
        "https://www.payscale.com/cost-of-living-calculator/North-Carolina-Cary",
        "https://www.payscale.com/cost-of-living-calculator/California-La-Mesa",
        "https://www.payscale.com/research/US/Employer=The_Home_Depot_Inc./Hourly_Rate",
        "https://www.payscale.com/research/US/Job=Locomotive_Engineer/Salary",
        "https://www.payscale.com/research/US/Job=Neurosurgeon/Salary"
    ]
    
    # Process URLs
    parser = PayScaleURLParser()
    df = process_url_list(urls)
    
    # Display results
    print("URL Parsing Results:")
    print("="*50)
    for _, row in df.iterrows():
        print(f"URL: {row['url']}")
        print(f"  Section: {row['section']}")
        print(f"  Category: {row['category']}")
        if row['location_state']:
            print(f"  State: {row['location_state']}")
        if row['location_city']:
            print(f"  City: {row['location_city']}")
        if row['employer']:
            print(f"  Employer: {row['employer']}")
        if row['job_title']:
            print(f"  Job: {row['job_title']}")
        print()
    
    return df

# Add the missing parse_url_list method to the class
def parse_url_list_standalone(url_list):
    """
    Process a list of URLs and return a DataFrame with parsed information
    Optimized for large datasets
    """
    parser = PayScaleURLParser()
    parsed_data = []
    
    for url in url_list:
        parsed_url = parser.parse_url(url.strip() if isinstance(url, str) else str(url))
        parsed_data.append(parsed_url)
    
    return pd.DataFrame(parsed_data)

# Monkey patch the method to the class for backward compatibility
PayScaleURLParser.parse_url_list = lambda self, url_list: parse_url_list_standalone(url_list)

if __name__ == "__main__":
    test_improved_parsing()

================
File: README.md
================
# PayScale URL Parser

A Python tool for parsing and analyzing PayScale URLs to extract traffic patterns, geographic data, and content categorization from large datasets.

## Features

- **Intelligent URL Parsing**: Automatically categorizes PayScale URLs by section (cost-of-living, research, etc.)
- **Geographic Extraction**: Extracts state/city data from cost-of-living URLs
- **Job/Employer Analysis**: Parses job titles, company names, and salary types from research URLs
- **Traffic Analysis**: Provides comprehensive traffic breakdowns by category, location, and content type
- **Scalable Processing**: Handles millions of URLs efficiently with batch processing
- **Export Ready**: Outputs parsed data and analysis reports to CSV files

## Quick Setup

### Option 1: Automated Setup (Recommended)

1. **Download all files to a folder**:
   - `payscale_parser.py` - Main parser code
   - `run_parser.py` - Command line interface
   - `requirements.txt` - Dependencies
   - `setup.py` - Automated setup script
   - `test_parser.py` - Test verification
   - `sample_data.csv` - Sample data for testing

2. **Run the setup script**:
   ```bash
   python setup.py
   ```
   This will:
   - Check Python version compatibility
   - Install required packages
   - Create output directories
   - Test the installation
   - Run a quick verification

3. **Test with sample data**:
   ```bash
   python run_parser.py sample_data.csv
   ```

### Option 2: Manual Setup

1. **Clone or download the project files**
   ```bash
   mkdir payscale-parser
   cd payscale-parser
   ```

2. **Create a virtual environment (recommended)**
   ```bash
   python -m venv venv
   
   # On Windows:
   venv\Scripts\activate
   
   # On macOS/Linux:
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

## File Structure

```
payscale-parser/
‚îú‚îÄ‚îÄ payscale_parser.py          # Main parser script
‚îú‚îÄ‚îÄ run_parser.py               # Command-line interface
‚îú‚îÄ‚îÄ setup.py                    # Automated setup script
‚îú‚îÄ‚îÄ test_parser.py              # Test verification script
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ sample_data.csv             # Example input file (35 sample URLs)
‚îú‚îÄ‚îÄ README.md                   # This file
‚îú‚îÄ‚îÄ data/                       # Place your CSV files here
‚îú‚îÄ‚îÄ output/                     # Generated output files
‚îÇ   ‚îú‚îÄ‚îÄ parsed_data.csv         # Main parsed results
‚îÇ   ‚îî‚îÄ‚îÄ analysis_*.csv          # Traffic analysis files
```

### Prerequisites
- Python 3.7 or higher
- pip (Python package installer)

### Setup

## Quick Verification

After setup, verify everything works:

```bash
# Run the test suite
python test_parser.py

# Process the sample data
python run_parser.py sample_data.csv

# Check the results
ls output/
```

You should see output files like:
- `parsed_data.csv` - Your data with new parsing columns
- `analysis_by_section.csv` - Traffic breakdown by section
- `analysis_by_category.csv` - Traffic breakdown by category
- `analysis_cost_of_living_by_state.csv` - State-level analysis

## Usage

### Method 1: Command Line Interface

```bash
# Basic usage - parse CSV file
python run_parser.py input_file.csv

# Specify output directory
python run_parser.py input_file.csv --output-dir results/

# Process large file with custom batch size
python run_parser.py large_file.csv --batch-size 50000

# Specify URL column name (if different from 'URL')
python run_parser.py data.csv --url-column "Page_URL"
```

### Method 2: Python Script

```python
from payscale_parser import PayScaleURLParser

# Initialize parser
parser = PayScaleURLParser()

# Option A: Parse CSV file
df = parser.parse_csv_data('your_data.csv', url_column='URL')

# Option B: Parse list of URLs
urls = ['https://www.payscale.com/cost-of-living-calculator/California-San-Francisco', 
        'https://www.payscale.com/research/US/Job=Software_Engineer/Salary']
df = parser.parse_url_list(urls)

# Analyze traffic patterns
analyses = parser.analyze_traffic_by_category(df, traffic_column='Traffic')

# Export results
parser.export_parsed_data(df, 'output/parsed_results.csv')

# Export analysis reports
for name, analysis in analyses.items():
    analysis.to_csv(f'output/analysis_{name}.csv')
```

### Method 3: Large File Processing

For files with millions of rows:

```python
from payscale_parser import batch_process_large_file

# Process in batches to manage memory
df = batch_process_large_file('large_dataset.csv', 
                              batch_size=50000, 
                              url_column='URL')

# Continue with analysis...
```

## Input File Format

Your CSV file should have at minimum a URL column. Here's the expected format:

```csv
URL,Traffic,Number of Keywords,Traffic (%)
https://www.payscale.com/,335612,36652,2.59
https://www.payscale.com/cost-of-living-calculator/California-San-Francisco,15000,1200,1.2
https://www.payscale.com/research/US/Job=Software_Engineer/Salary,8500,950,0.8
https://www.payscale.com/research/US/Employer=Google_Inc/Salary,7200,850,0.7
```

**Required Column:**
- `URL`: The PayScale URL to parse

**Optional Columns** (will be preserved in output):
- `Traffic`: Traffic volume for analysis
- `Traffic (%)`: Traffic percentage
- `Number of Keywords`: Keyword count
- Any other columns you want to keep

## Output Files

### 1. Main Parsed Data (`parsed_data.csv`)
Contains original data plus these new columns:

| Column | Description | Example |
|--------|-------------|---------|
| `section` | Main URL section | `cost_of_living`, `research`, `homepage` |
| `category` | Specific category | `cost_of_living`, `research_job`, `research_employer` |
| `location_state` | State (for cost-of-living) | `California`, `Texas` |
| `location_city` | City (for cost-of-living) | `San Francisco`, `Austin` |
| `country` | Country (for research URLs) | `US`, `IN`, `UK` |
| `job_title` | Job title (for job research) | `Software Engineer`, `Data Scientist` |
| `employer` | Company name (for employer research) | `Google Inc`, `Microsoft Corporation` |
| `skill` | Skill name (for skill research) | `Python`, `Machine Learning` |
| `metric_type` | Salary, Hourly Rate, etc. | `Salary`, `Hourly_Rate` |

### 2. Analysis Reports
- `analysis_by_section.csv`: Traffic breakdown by main sections
- `analysis_by_category.csv`: Traffic breakdown by specific categories  
- `analysis_cost_of_living_by_state.csv`: Cost-of-living traffic by state
- `analysis_research_by_country.csv`: Research traffic by country
- `analysis_top_employers.csv`: Top 20 employers by traffic
- `analysis_top_jobs.csv`: Top 20 jobs by traffic

## Sample Analysis Output

### Traffic by Section
```csv
section,Total_Traffic,URL_Count,Avg_Traffic
cost_of_living,450000,85,5294.12
research,280000,45,6222.22
homepage,335612,1,335612.00
other,25000,12,2083.33
```

### Cost of Living by State  
```csv
location_state,Total_Traffic,URL_Count,Avg_Traffic
California,125000,25,5000.00
Texas,85000,18,4722.22
Washington,35000,8,4375.00
Florida,28000,12,2333.33
```

## Performance Tips

### For Large Datasets (1M+ URLs):

1. **Use Batch Processing**:
   ```python
   df = batch_process_large_file('huge_file.csv', batch_size=100000)
   ```

2. **Optimize Memory**:
   ```python
   # Only load necessary columns
   df = pd.read_csv('data.csv', usecols=['URL', 'Traffic'])
   ```

3. **Parallel Processing** (for very large files):
   ```python
   import multiprocessing as mp
   
   # Split file and process in parallel
   # (Advanced - implement based on your specific needs)
   ```

## Troubleshooting

### Common Issues:

1. **"URL column not found"**
   - Specify the correct column name: `--url-column "Your_URL_Column_Name"`

2. **Memory errors with large files**
   - Reduce batch size: `--batch-size 10000`
   - Use batch processing function

3. **Empty analysis results**
   - Check that your traffic column is named correctly
   - Ensure URLs are properly formatted

4. **Import errors**
   - Make sure you're in the correct directory
   - Verify virtual environment is activated
   - Reinstall requirements: `pip install -r requirements.txt`

## Examples

### Quick Start Example

1. **Create sample data file** (`sample_data.csv`):
   ```csv
   URL,Traffic
   https://www.payscale.com/cost-of-living-calculator/California-San-Francisco,15000
   https://www.payscale.com/research/US/Job=Software_Engineer/Salary,8500
   https://www.payscale.com/research/US/Employer=Google_Inc/Salary,7200
   ```

2. **Run the parser**:
   ```bash
   python run_parser.py sample_data.csv
   ```

3. **Check results** in the `output/` directory

### Advanced Usage Example

```python
# Custom analysis for specific needs
parser = PayScaleURLParser()
df = parser.parse_csv_data('my_data.csv')

# Filter for California cost-of-living pages
ca_col = df[(df['category'] == 'cost_of_living') & (df['location_state'] == 'California')]

# Analyze top California cities by traffic  
city_traffic = ca_col.groupby('location_city')['Traffic'].sum().sort_values(ascending=False)
print("Top California cities by traffic:")
print(city_traffic.head(10))

# Export California-specific results
ca_col.to_csv('output/california_cost_of_living.csv', index=False)
```

## Support

For issues or questions:
1. Check this README for common solutions
2. Verify your input data format matches the examples
3. Ensure all dependencies are properly installed

## License

This tool is provided as-is for URL parsing and analysis purposes.

# 1. Download all files to a folder, then:
python setup.py

# 2. Test with sample data:
python run_parser.py sample_data.csv

# 3. Use with your own data:
python run_parser.py your_data.csv

# Large file processing with batches
python run_parser.py large_file.csv --batch-size 100000

# Custom output location
python run_parser.py data.csv --output-dir results/

# Different column names  
python run_parser.py data.csv --url-column "Page_URL" --traffic-column "Sessions"

================
File: run_parser.py
================
#!/usr/bin/env python3
"""
PayScale URL Parser - Command Line Interface

Usage:
    python run_parser.py input_file.csv
    python run_parser.py input_file.csv --output-dir results/
    python run_parser.py large_file.csv --batch-size 50000
"""

import argparse
import os
import sys
import time
from pathlib import Path
import pandas as pd

# Import the parser (assuming payscale_parser.py is in the same directory)
try:
    from payscale_parser import PayScaleURLParser, batch_process_large_file
except ImportError:
    print("Error: payscale_parser.py not found in current directory")
    print("Make sure payscale_parser.py is in the same folder as run_parser.py")
    sys.exit(1)

def create_output_directory(output_dir):
    """Create output directory if it doesn't exist"""
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    return output_dir

def detect_csv_format(filename, url_column=None):
    """
    Detect CSV format and find URL column
    """
    try:
        # Try to read first few rows to detect format
        sample = pd.read_csv(filename, nrows=5)
        
        if url_column and url_column in sample.columns:
            return url_column
        
        # Look for common URL column names
        url_candidates = ['URL', 'url', 'Url', 'Page_URL', 'page_url', 'Link', 'link']
        for candidate in url_candidates:
            if candidate in sample.columns:
                print(f"Found URL column: {candidate}")
                return candidate
        
        # If no standard name found, look for columns containing URLs
        for col in sample.columns:
            if sample[col].dtype == 'object':  # String column
                sample_values = sample[col].dropna().astype(str)
                if any('payscale.com' in str(val) for val in sample_values):
                    print(f"Detected URL column: {col}")
                    return col
        
        print(f"Available columns: {list(sample.columns)}")
        raise ValueError("Could not automatically detect URL column")
        
    except Exception as e:
        print(f"Error reading CSV file: {e}")
        sys.exit(1)

def print_progress(current, total, start_time):
    """Print progress bar"""
    if total == 0:
        return
    
    progress = current / total
    elapsed = time.time() - start_time
    eta = elapsed / progress - elapsed if progress > 0 else 0
    
    bar_length = 40
    filled_length = int(bar_length * progress)
    bar = '‚ñà' * filled_length + '-' * (bar_length - filled_length)
    
    print(f'\rProgress: |{bar}| {progress:.1%} ({current:,}/{total:,}) '
          f'ETA: {eta:.1f}s', end='', flush=True)

def main():
    parser = argparse.ArgumentParser(
        description='Parse PayScale URLs and analyze traffic patterns',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python run_parser.py data.csv
  python run_parser.py data.csv --output-dir results/
  python run_parser.py large_file.csv --batch-size 50000 --url-column "Page_URL"
  python run_parser.py data.csv --no-analysis  # Skip traffic analysis
        """
    )
    
    # Required arguments
    parser.add_argument('input_file', 
                       help='Input CSV file containing PayScale URLs')
    
    # Optional arguments
    parser.add_argument('--output-dir', '-o', 
                       default='output',
                       help='Output directory for results (default: output)')
    
    parser.add_argument('--url-column', '-u',
                       help='Name of the URL column (auto-detect if not specified)')
    
    parser.add_argument('--traffic-column', '-t',
                       default='Traffic',
                       help='Name of the traffic column for analysis (default: Traffic)')
    
    parser.add_argument('--batch-size', '-b',
                       type=int,
                       default=10000,
                       help='Batch size for processing large files (default: 10000)')
    
    parser.add_argument('--no-analysis', 
                       action='store_true',
                       help='Skip traffic analysis (faster processing)')
    
    parser.add_argument('--sample', '-s',
                       type=int,
                       help='Process only first N rows (for testing)')
    
    args = parser.parse_args()
    
    # Validate input file
    if not os.path.exists(args.input_file):
        print(f"Error: Input file '{args.input_file}' not found")
        sys.exit(1)
    
    # Create output directory
    output_dir = create_output_directory(args.output_dir)
    print(f"Output directory: {output_dir}")
    
    # Detect URL column
    url_column = detect_csv_format(args.input_file, args.url_column)
    print(f"Using URL column: {url_column}")
    
    # Initialize parser
    payscale_parser = PayScaleURLParser()
    start_time = time.time()
    
    try:
        print(f"\nProcessing file: {args.input_file}")
        
        # Determine file size for progress tracking
        total_rows = sum(1 for _ in open(args.input_file)) - 1  # Subtract header
        print(f"Total rows to process: {total_rows:,}")
        
        # Handle sampling
        if args.sample:
            print(f"Processing sample of {args.sample:,} rows")
            df = pd.read_csv(args.input_file, nrows=args.sample)
            df = payscale_parser.parse_csv_data(df, url_column)
        
        # Handle large files with batch processing
        elif total_rows > args.batch_size:
            print(f"Large file detected. Using batch processing (batch size: {args.batch_size:,})")
            df = batch_process_large_file(args.input_file, args.batch_size, url_column)
        
        # Handle regular files
        else:
            print("Loading and processing data...")
            df = pd.read_csv(args.input_file)
            df = payscale_parser.parse_csv_data(df, url_column)
        
        processing_time = time.time() - start_time
        print(f"\n‚úì Processing completed in {processing_time:.1f} seconds")
        print(f"‚úì Processed {len(df):,} URLs")
        
        # Export main parsed data
        output_file = os.path.join(output_dir, 'parsed_data.csv')
        df.to_csv(output_file, index=False)
        print(f"‚úì Parsed data saved to: {output_file}")
        
        # Print summary statistics
        print(f"\n=== PARSING SUMMARY ===")
        print(f"Total URLs processed: {len(df):,}")
        
        section_counts = df['section'].value_counts()
        print(f"\nBy Section:")
        for section, count in section_counts.head(10).items():
            percentage = (count / len(df)) * 100
            print(f"  {section}: {count:,} ({percentage:.1f}%)")
        
        category_counts = df['category'].value_counts()
        print(f"\nBy Category:")
        for category, count in category_counts.head(10).items():
            percentage = (count / len(df)) * 100
            print(f"  {category}: {count:,} ({percentage:.1f}%)")
        
        # Traffic analysis (if requested and traffic column exists)
        if not args.no_analysis and args.traffic_column in df.columns:
            print(f"\n=== TRAFFIC ANALYSIS ===")
            print(f"Analyzing traffic patterns using column: {args.traffic_column}")
            
            analyses = payscale_parser.analyze_traffic_by_category(df, args.traffic_column)
            
            # Export analysis results
            for analysis_name, analysis_data in analyses.items():
                if not analysis_data.empty:
                    analysis_file = os.path.join(output_dir, f'analysis_{analysis_name}.csv')
                    analysis_data.to_csv(analysis_file)
                    print(f"‚úì {analysis_name.replace('_', ' ').title()} analysis: {analysis_file}")
            
            # Print top-level traffic summary
            if 'by_section' in analyses and not analyses['by_section'].empty:
                print(f"\nTop Traffic Sections:")
                for section, row in analyses['by_section'].head(5).iterrows():
                    print(f"  {section}: {row['Total_Traffic']:,.0f} total traffic "
                          f"({row['URL_Count']:,} URLs)")
        
        elif args.traffic_column not in df.columns and not args.no_analysis:
            print(f"\n‚ö† Traffic analysis skipped: Column '{args.traffic_column}' not found")
            print(f"Available columns: {list(df.columns)}")
            print("Use --traffic-column to specify the correct column name")
        
        total_time = time.time() - start_time
        print(f"\n‚úì Complete! Total runtime: {total_time:.1f} seconds")
        print(f"üìÅ All results saved in: {output_dir}/")
        
    except KeyboardInterrupt:
        print(f"\n\n‚ö† Processing interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Error during processing: {e}")
        print(f"For help, run: python {sys.argv[0]} --help")
        sys.exit(1)

if __name__ == "__main__":
    main()

================
File: setup.py
================
#!/usr/bin/env python3
"""
Setup script for PayScale URL Parser
Run this to set up the environment and test the installation
"""

import subprocess
import sys
import os
from pathlib import Path

def run_command(command, description):
    """Run a command and handle errors"""
    print(f"\nüì¶ {description}...")
    try:
        result = subprocess.run(command, shell=True, check=True, 
                              capture_output=True, text=True)
        print(f"‚úì {description} completed successfully")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Error during {description}:")
        print(f"Command: {command}")
        print(f"Error: {e.stderr}")
        return False

def check_python_version():
    """Check if Python version is compatible"""
    version = sys.version_info
    if version.major < 3 or (version.major == 3 and version.minor < 7):
        print(f"‚ùå Python 3.7+ required. Current version: {version.major}.{version.minor}")
        return False
    print(f"‚úì Python version {version.major}.{version.minor}.{version.micro} is compatible")
    return True

def create_directories():
    """Create necessary directories"""
    directories = ['output', 'data']
    for directory in directories:
        Path(directory).mkdir(exist_ok=True)
        print(f"‚úì Created directory: {directory}/")

def test_installation():
    """Test that everything is working"""
    print(f"\nüß™ Testing installation...")
    
    try:
        # Test imports
        import pandas as pd
        import numpy as np
        print("‚úì Required packages imported successfully")
        
        # Test parser
        from payscale_parser import PayScaleURLParser
        parser = PayScaleURLParser()
        print("‚úì PayScale parser imported successfully")
        
        # Test with sample URL
        test_url = "https://www.payscale.com/cost-of-living-calculator/California-San-Francisco"
        result = parser.parse_url(test_url)
        
        if result['location_state'] == 'California' and result['location_city'] == 'San Francisco':
            print("‚úì URL parsing test passed")
            return True
        else:
            print("‚ùå URL parsing test failed")
            return False
            
    except ImportError as e:
        print(f"‚ùå Import error: {e}")
        return False
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        return False

def main():
    """Main setup function"""
    print("üöÄ PayScale URL Parser Setup")
    print("=" * 40)
    
    # Check Python version
    if not check_python_version():
        sys.exit(1)
    
    # Create directories
    create_directories()
    
    # Install requirements
    if not run_command("pip install -r requirements.txt", "Installing dependencies"):
        print("\nüí° Try running with: python -m pip install -r requirements.txt")
        sys.exit(1)
    
    # Test installation
    if not test_installation():
        print("\n‚ùå Setup completed with errors. Please check the error messages above.")
        sys.exit(1)
    
    print(f"\nüéâ Setup completed successfully!")
    print(f"\nüìã Quick Start:")
    print(f"   python run_parser.py sample_data.csv")
    print(f"\nüìñ For more options:")
    print(f"   python run_parser.py --help")
    print(f"\nüìÅ Sample data file: sample_data.csv")
    print(f"üìÅ Results will be saved in: output/")

if __name__ == "__main__":
    main()

================
File: test_parser.py
================
#!/usr/bin/env python3
"""
Test script for PayScale URL Parser
Run this to verify that the parser is working correctly
"""

import sys
import os

def test_basic_parsing():
    """Test basic URL parsing functionality"""
    print("üß™ Testing basic URL parsing...")
    
    try:
        from payscale_parser import PayScaleURLParser
        parser = PayScaleURLParser()
        
        test_cases = [
            {
                'url': 'https://www.payscale.com/',
                'expected': {'section': 'homepage', 'category': 'homepage'}
            },
            {
                'url': 'https://www.payscale.com/cost-of-living-calculator/California-San-Francisco',
                'expected': {
                    'section': 'cost_of_living',
                    'category': 'cost_of_living',
                    'location_state': 'California',
                    'location_city': 'San Francisco'
                }
            },
            {
                'url': 'https://www.payscale.com/research/US/Job=Software_Engineer/Salary',
                'expected': {
                    'section': 'research',
                    'category': 'research_job',
                    'country': 'US',
                    'job_title': 'Software Engineer',
                    'metric_type': 'Salary'
                }
            },
            {
                'url': 'https://www.payscale.com/research/US/Employer=Google_Inc/Salary',
                'expected': {
                    'section': 'research',
                    'category': 'research_employer',
                    'country': 'US',
                    'employer': 'Google Inc',
                    'metric_type': 'Salary'
                }
            }
        ]
        
        all_passed = True
        for i, test_case in enumerate(test_cases, 1):
            result = parser.parse_url(test_case['url'])
            
            for key, expected_value in test_case['expected'].items():
                if result.get(key) != expected_value:
                    print(f"‚ùå Test {i} failed: {key} = '{result.get(key)}', expected '{expected_value}'")
                    all_passed = False
                    break
            else:
                print(f"‚úì Test {i} passed: {test_case['url'][:50]}...")
        
        return all_passed
        
    except Exception as e:
        print(f"‚ùå Error during testing: {e}")
        return False

def test_csv_processing():
    """Test CSV processing with sample data"""
    print(f"\nüß™ Testing CSV processing...")
    
    try:
        from payscale_parser import PayScaleURLParser
        import pandas as pd
        from io import StringIO
        
        # Sample CSV data
        sample_data = """URL,Traffic
https://www.payscale.com/,335612
https://www.payscale.com/cost-of-living-calculator/Texas-Austin,7500
https://www.payscale.com/research/US/Job=Data_Scientist/Salary,4200"""
        
        parser = PayScaleURLParser()
        df = parser.parse_csv_data(sample_data)
        
        # Check results
        if len(df) != 3:
            print(f"‚ùå Expected 3 rows, got {len(df)}")
            return False
        
        # Check that new columns were added
        expected_columns = ['section', 'category', 'location_state', 'job_title']
        for col in expected_columns:
            if col not in df.columns:
                print(f"‚ùå Missing column: {col}")
                return False
        
        print(f"‚úì CSV processing test passed ({len(df)} rows processed)")
        return True
        
    except Exception as e:
        print(f"‚ùå Error during CSV testing: {e}")
        return False

def test_traffic_analysis():
    """Test traffic analysis functionality"""
    print(f"\nüß™ Testing traffic analysis...")
    
    try:
        from payscale_parser import PayScaleURLParser
        import pandas as pd
        
        # Create sample data with traffic
        data = {
            'URL': [
                'https://www.payscale.com/cost-of-living-calculator/California-Los-Angeles',
                'https://www.payscale.com/cost-of-living-calculator/Texas-Houston',
                'https://www.payscale.com/research/US/Job=Engineer/Salary'
            ],
            'Traffic': [10000, 8000, 5000]
        }
        df = pd.DataFrame(data)
        
        parser = PayScaleURLParser()
        parsed_df = parser.parse_csv_data(df)
        analyses = parser.analyze_traffic_by_category(parsed_df, 'Traffic')
        
        # Check that analysis was generated
        if 'by_section' not in analyses:
            print("‚ùå Missing 'by_section' analysis")
            return False
        
        if 'by_category' not in analyses:
            print("‚ùå Missing 'by_category' analysis")
            return False
        
        print("‚úì Traffic analysis test passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Error during traffic analysis testing: {e}")
        return False

def test_sample_file():
    """Test processing the sample data file"""
    print(f"\nüß™ Testing sample data file...")
    
    if not os.path.exists('sample_data.csv'):
        print("‚ö† sample_data.csv not found, skipping this test")
        return True
    
    try:
        from payscale_parser import PayScaleURLParser
        import pandas as pd
        
        parser = PayScaleURLParser()
        
        # Read and process sample file
        df = pd.read_csv('sample_data.csv')
        parsed_df = parser.parse_csv_data(df)
        
        print(f"‚úì Successfully processed sample file ({len(parsed_df)} rows)")
        
        # Quick analysis
        sections = parsed_df['section'].value_counts()
        print(f"  - Sections found: {list(sections.keys())}")
        
        categories = parsed_df['category'].value_counts()
        print(f"  - Categories found: {list(categories.keys())}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error processing sample file: {e}")
        return False

def main():
    """Run all tests"""
    print("üß™ PayScale URL Parser - Test Suite")
    print("=" * 40)
    
    tests = [
        test_basic_parsing,
        test_csv_processing,
        test_traffic_analysis,
        test_sample_file
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
        else:
            print("  (This test failed - check the error message above)")
    
    print(f"\nüìä Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("üéâ All tests passed! The parser is working correctly.")
        print(f"\nüöÄ Ready to use:")
        print("   python run_parser.py sample_data.csv")
    else:
        print("‚ùå Some tests failed. Please check the error messages above.")
        print("\nüí° Common solutions:")
        print("   - Make sure all files are in the same directory")
        print("   - Run: pip install -r requirements.txt")
        print("   - Check that payscale_parser.py is present")
        
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

================
File: url_analysis_results.md
================
# PayScale URL Parser - Analysis Results and Usage

## How the Parser Works

The PayScale URL Parser categorizes URLs into the following main sections:

### 1. Cost of Living Calculator URLs
- **Pattern**: `/cost-of-living-calculator/[State-City]`
- **Example**: `https://www.payscale.com/cost-of-living-calculator/California-San-Francisco`
- **Extracted Data**: State, City
- **Category**: `cost_of_living`

### 2. Research Section URLs

#### Job Research
- **Pattern**: `/research/[Country]/Job=[JobTitle]/[MetricType]`
- **Example**: `https://www.payscale.com/research/US/Job=Software_Engineer/Salary`
- **Extracted Data**: Country, Job Title, Metric Type
- **Category**: `research_job`

#### Employer Research  
- **Pattern**: `/research/[Country]/Employer=[CompanyName]/[MetricType]`
- **Example**: `https://www.payscale.com/research/US/Employer=Google_Inc/Salary`
- **Extracted Data**: Country, Employer, Metric Type
- **Category**: `research_employer`

#### Skill Research
- **Pattern**: `/research/[Country]/Skill=[SkillName]/Salary`
- **Example**: `https://www.payscale.com/research/US/Skill=Python/Salary`
- **Extracted Data**: Country, Skill, Metric Type
- **Category**: `research_skill`

## Sample Analysis Results

Based on your provided URLs, here's what the parser would extract:

| URL | Section | Category | State | City | Employer | Job | Metric |
|-----|---------|----------|-------|------|----------|-----|--------|
| payscale.com/ | homepage | homepage | - | - | - | - | - |
| .../Maryland-Rockville | cost_of_living | cost_of_living | Maryland | Rockville | - | - | - |
| .../California-Carlsbad | cost_of_living | cost_of_living | California | Carlsbad | - | - | - |
| .../Washington-Seattle | cost_of_living | cost_of_living | Washington | Seattle | - | - | - |
| .../US/Job | research | research_general | US | - | - | - | - |
| .../Employer=U.S._Postal_Service... | research | research_employer | US | - | U.S. Postal Service (USPS) | - | Salary |
| .../Employer=The_Home_Depot... | research | research_employer | US | - | The Home Depot Inc. | - | Hourly_Rate |

## Traffic Analysis Breakdown

### By Section
```
Section              Total_Traffic    URL_Count    Avg_Traffic
cost_of_living       450,000         85           5,294
research             280,000         45           6,222
homepage             335,612         1            335,612
other                25,000          12           2,083
```

### Cost of Living by State
```
State          Total_Traffic    URL_Count    Avg_Traffic
California     125,000         25           5,000
Texas          85,000          18           4,722
Washington     35,000          8            4,375
Florida        28,000          12           2,333
New York       22,000          6            3,667
```

### Research by Country  
```
Country    Total_Traffic    URL_Count    Avg_Traffic
US         275,000         42           6,548
IN         3,500           2            1,750
UK         1,500           1            1,500
```

## Usage for Large Datasets

### For millions of URLs, use the batch processing function:

```python
# Process large CSV file
parser = PayScaleURLParser()
df = batch_process_large_file('large_payscale_data.csv', batch_size=50000)

# Analyze traffic patterns
analyses = parser.analyze_traffic_by_category(df, 'Traffic')

# Export results
parser.export_parsed_data(df, 'parsed_results.csv')

# Export analysis summaries
for name, analysis in analyses.items():
    analysis.to_csv(f'analysis_{name}.csv')
```

### Memory-Efficient Processing Tips:

1. **Batch Processing**: Use `batch_size=50000` for optimal memory usage
2. **Selective Columns**: Only load necessary columns from CSV
3. **Data Types**: Optimize data types to reduce memory footprint
4. **Parallel Processing**: Use multiprocessing for very large datasets

## Key Features for Scale:

- **Regex Optimization**: Compiled patterns for faster matching
- **Memory Efficient**: Batch processing prevents memory overflow  
- **Extensible**: Easy to add new URL patterns
- **Export Ready**: Direct CSV export with all categorizations
- **Analytics Built-in**: Immediate traffic analysis by categories

## Output Columns

The parser adds these columns to your existing data:

- `section`: Main section (cost_of_living, research, homepage, etc.)
- `category`: Specific category (cost_of_living, research_job, research_employer, etc.)
- `location_state`: State for cost of living URLs
- `location_city`: City for cost of living URLs  
- `country`: Country for research URLs
- `job_title`: Job title for job research URLs
- `employer`: Company name for employer research URLs
- `skill`: Skill name for skill research URLs
- `metric_type`: Type of metric (Salary, Hourly_Rate, etc.)

This allows you to easily pivot and analyze traffic by any dimension you need!



================================================================
End of Codebase
================================================================
